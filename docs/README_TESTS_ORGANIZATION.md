# ğŸ§ª Organisation des Tests - Dashboard DÃ©rivÃ©s Actions

## ğŸ“‹ Vue d'ensemble

Ce document dÃ©crit l'organisation et la structure des tests dans le projet **Dashboard DÃ©rivÃ©s Actions**. Tous les tests sont organisÃ©s dans le dossier `tests/` pour une meilleure organisation et maintenance.

## ğŸ“ Structure du Dossier Tests

```
tests/
â”œâ”€â”€ __init__.py              # Package Python
â”œâ”€â”€ README.md                # Documentation principale des tests
â”œâ”€â”€ README_TESTS.md          # Documentation dÃ©taillÃ©e des tests
â”œâ”€â”€ RESUME_TESTS.md          # RÃ©sumÃ© et mÃ©triques des tests
â”œâ”€â”€ test_app.py              # Fichier principal de tests (40 tests)
â”œâ”€â”€ run_tests.py             # Script d'exÃ©cution automatique
â”œâ”€â”€ pytest.ini              # Configuration pytest
â””â”€â”€ .pytest_cache/          # Cache pytest (gÃ©nÃ©rÃ© automatiquement)
```

## ğŸ¯ Organisation des Tests

### 1. **test_app.py** - Fichier Principal de Tests
Contient 40 tests organisÃ©s en 3 classes :

#### `FlaskAppTestCase` (35 tests)
- **Tests de Pages Web** (8 tests)
  - Page d'accueil, CV, indices, call/put, volatilitÃ©, debug, analyse
- **Tests d'APIs** (15 tests)
  - Market data, chart data, calculate option, risk metrics, etc.
- **Tests de Calculs Financiers** (8 tests)
  - Options call/put, grecques, Monte Carlo, Black-Scholes
- **Tests de Validation** (4 tests)
  - ParamÃ¨tres invalides, erreurs 404, routes legacy

#### `PerformanceTestCase` (2 tests)
- **Temps de calcul** < 10 secondes
- **RequÃªtes concurrentes** (5 simultanÃ©es)

#### `ErrorHandlingTestCase` (3 tests)
- **JSON invalide**
- **Content-Type manquant**
- **Valeurs extrÃªmes**

### 2. **run_tests.py** - Script d'ExÃ©cution
Interface en ligne de commande avec options :
- `--fast` : Tests rapides seulement
- `--performance` : Tests de performance
- `--unit` : Tests unitaires
- `--integration` : Tests d'intÃ©gration
- `--coverage` : Tests avec couverture de code

### 3. **pytest.ini** - Configuration
Configuration automatique pour pytest :
- Mode verbeux par dÃ©faut
- Gestion des marqueurs
- Filtrage des avertissements
- Couleurs activÃ©es

## ğŸš€ ExÃ©cution des Tests

### Depuis le Dossier Racine
```bash
# Tous les tests
python tests/run_tests.py

# Tests rapides seulement
python tests/run_tests.py --fast

# Tests de performance seulement
python tests/run_tests.py --performance

# Tests avec couverture de code
python tests/run_tests.py --coverage
```

### Depuis le Dossier Tests
```bash
cd tests

# Tous les tests
python run_tests.py

# Tests spÃ©cifiques
python run_tests.py --unit
python run_tests.py --integration
```

### Commandes Directes
```bash
cd tests

# Avec unittest
python test_app.py

# Avec pytest
python -m pytest test_app.py -v

# Tests spÃ©cifiques
python -m pytest test_app.py::FlaskAppTestCase -v
python -m pytest test_app.py::PerformanceTestCase -v
```

## ğŸ“Š MÃ©triques de Test

### Taux de RÃ©ussite : 100%
- **40 tests** exÃ©cutÃ©s avec succÃ¨s
- **0 Ã©chec**
- **0 erreur**
- **Temps d'exÃ©cution** : ~8-10 secondes

### Couverture de Code
```bash
python tests/run_tests.py --coverage
```
- **Tests unitaires** : 35 tests
- **Tests d'intÃ©gration** : 3 tests
- **Tests de performance** : 2 tests
- **Couverture estimÃ©e** : >90%

### Performance
- **Temps de calcul d'options** : <10 secondes âœ…
- **RequÃªtes concurrentes** : 5 simultanÃ©es âœ…
- **Temps de rÃ©ponse API** : <1 seconde âœ…

## ğŸ”§ Configuration Technique

### Import des Modules
Les tests utilisent un chemin relatif pour importer l'application :
```python
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from app import app
```

### Mocking des APIs
Les tests utilisent `unittest.mock` pour simuler :
- âœ… **Yahoo Finance API** : DonnÃ©es de marchÃ©
- âœ… **Finnhub API** : DonnÃ©es d'options
- âœ… **Polygon.io API** : DonnÃ©es avancÃ©es

Cela permet de tester l'application sans dÃ©pendre des APIs externes.

### Configuration Pytest
```ini
[tool:pytest]
addopts = -v --tb=short --strict-markers
markers =
    slow: marks tests as slow (deselect with '-m "not slow"')
    integration: marks tests as integration tests
    unit: marks tests as unit tests
    performance: marks tests as performance tests
testpaths = .
python_files = test_*.py
python_classes = Test*
python_functions = test_*
```

## ğŸ¯ Types de Tests

### 1. Tests Unitaires
- **Objectif** : Tester des fonctionnalitÃ©s individuelles
- **Exemples** : Calculs d'options, grecques, APIs
- **Isolation** : Chaque test est indÃ©pendant
- **Mocking** : APIs externes mockÃ©es

### 2. Tests d'IntÃ©gration
- **Objectif** : Tester les interactions entre composants
- **Exemples** : Flux complet d'une requÃªte API
- **DÃ©pendances** : Peut utiliser plusieurs modules
- **Validation** : VÃ©rifier le comportement global

### 3. Tests de Performance
- **Objectif** : VÃ©rifier les temps de rÃ©ponse
- **Exemples** : Calculs lourds, requÃªtes concurrentes
- **MÃ©triques** : Temps d'exÃ©cution, charge
- **Seuils** : DÃ©finir des limites acceptables

### 4. Tests de Gestion d'Erreurs
- **Objectif** : VÃ©rifier la robustesse
- **Exemples** : DonnÃ©es invalides, erreurs API
- **Validation** : Messages d'erreur appropriÃ©s
- **RÃ©cupÃ©ration** : Gestion gracieuse des erreurs

## ğŸ“ Ajout de Nouveaux Tests

### 1. Test de Page Web
```python
def test_nouvelle_page(self):
    """Test de la nouvelle page"""
    response = self.app.get('/nouvelle-page')
    self.assertEqual(response.status_code, 200)
    self.assertIn(b'<!DOCTYPE html>', response.data)
```

### 2. Test d'API
```python
@patch('api.external_api.get_data')
def test_nouvelle_api(self, mock_get_data):
    """Test de la nouvelle API"""
    mock_get_data.return_value = {'test': 'data'}
    
    response = self.app.get('/api/nouvelle-api')
    self.assertEqual(response.status_code, 200)
    data = json.loads(response.data)
    self.assertIn('test', data)
```

### 3. Test de Calcul
```python
def test_nouveau_calcul(self):
    """Test du nouveau calcul"""
    test_data = {
        'param1': 100,
        'param2': 0.2
    }
    
    response = self.app.post('/api/calcul',
                           data=json.dumps(test_data),
                           content_type='application/json')
    
    self.assertEqual(response.status_code, 200)
    result = json.loads(response.data)
    self.assertIn('resultat', result)
```

### 4. Test de Performance
```python
def test_performance_nouveau_calcul(self):
    """Test de performance du nouveau calcul"""
    start_time = time.time()
    
    response = self.app.post('/api/calcul-lourd', ...)
    
    end_time = time.time()
    self.assertLess(end_time - start_time, 5.0)  # < 5 secondes
    self.assertEqual(response.status_code, 200)
```

## ğŸ¯ Bonnes Pratiques

### 1. Nommage des Tests
- `test_<fonctionnalitÃ©>_<scÃ©nario>` : `test_api_calculate_option_valid_data`
- `test_<fonctionnalitÃ©>_<erreur>` : `test_api_calculate_option_invalid_data`
- `test_<fonctionnalitÃ©>_<performance>` : `test_api_calculate_option_performance`

### 2. Structure AAA
- **Arrange** : PrÃ©parer les donnÃ©es de test
- **Act** : ExÃ©cuter l'action Ã  tester
- **Assert** : VÃ©rifier les rÃ©sultats

### 3. Isolation
- Chaque test doit Ãªtre indÃ©pendant
- Utiliser `setUp()` et `tearDown()` pour la configuration
- Nettoyer les donnÃ©es aprÃ¨s chaque test

### 4. Mocking
- Mocker les APIs externes
- Mocker les appels de base de donnÃ©es
- Mocker les fonctions coÃ»teuses

### 5. Documentation
- Documenter chaque test avec une docstring claire
- Expliquer le but et les conditions du test
- Inclure des exemples si nÃ©cessaire

## ğŸ”„ IntÃ©gration Continue

### GitHub Actions
```yaml
name: Tests
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.9
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Run tests
        run: python tests/run_tests.py
      - name: Upload coverage
        uses: codecov/codecov-action@v1
        with:
          file: ./tests/htmlcov/coverage.xml
```

### PrÃ©-commit Hooks
```yaml
# .pre-commit-config.yaml
repos:
  - repo: local
    hooks:
      - id: run-tests
        name: Run Tests
        entry: python tests/run_tests.py --fast
        language: system
        pass_filenames: false
```

## ğŸ“Š Monitoring et Rapports

### Rapports de Couverture
```bash
python tests/run_tests.py --coverage
```
GÃ©nÃ¨re :
- Rapport console avec pourcentage de couverture
- Rapport HTML dans `tests/htmlcov/index.html`

### MÃ©triques de Performance
- Temps d'exÃ©cution des tests
- Temps de rÃ©ponse des APIs
- Utilisation mÃ©moire
- Charge CPU

### Alertes
- Tests qui Ã©chouent
- Couverture qui baisse
- Performance qui se dÃ©grade
- Nouvelles fonctionnalitÃ©s sans tests

## ğŸ› DÃ©pannage

### Erreurs Courantes

#### 1. ModuleNotFoundError
```bash
# Installer les dÃ©pendances
pip install flask pandas numpy scipy requests pytest coverage
```

#### 2. ImportError pour les modules locaux
```bash
# VÃ©rifier que vous Ãªtes dans le bon rÃ©pertoire
ls app.py tests/test_app.py
```

#### 3. Tests qui Ã©chouent
```bash
# VÃ©rifier les logs dÃ©taillÃ©s
cd tests
python -m pytest test_app.py -v -s
```

### Debug des Tests
```bash
cd tests

# Mode debug avec pytest
python -m pytest test_app.py -v -s --pdb

# Test spÃ©cifique avec debug
python -m pytest test_app.py::FlaskAppTestCase::test_api_calculate_option_valid_data -v -s --pdb
```

## ğŸ“š Documentation ComplÃ¨te

- **[tests/README.md](../tests/README.md)** - Documentation principale des tests
- **[tests/README_TESTS.md](../tests/README_TESTS.md)** - Documentation dÃ©taillÃ©e
- **[tests/RESUME_TESTS.md](../tests/RESUME_TESTS.md)** - RÃ©sumÃ© et mÃ©triques

## ğŸ“ Support

En cas de problÃ¨me avec les tests :
1. VÃ©rifier que toutes les dÃ©pendances sont installÃ©es
2. VÃ©rifier que vous Ãªtes dans le bon rÃ©pertoire
3. ExÃ©cuter avec `-v` pour plus de dÃ©tails
4. Consulter les logs d'erreur spÃ©cifiques
5. VÃ©rifier la configuration pytest

---

**Auteur** : Mathis Le Gall  
**Date** : 2024  
**Version** : 1.0  
**Statut** : âœ… TerminÃ© et ValidÃ©

