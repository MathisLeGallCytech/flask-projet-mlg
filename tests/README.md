# ğŸ§ª Tests - Dashboard DÃ©rivÃ©s Actions

## ğŸ“‹ Vue d'ensemble

Ce dossier contient tous les tests pour l'application Flask **Dashboard DÃ©rivÃ©s Actions**. Le systÃ¨me de tests est complet et couvre toutes les fonctionnalitÃ©s de l'application.

## ğŸŒ AccÃ¨s Ã  l'Application

**Application dÃ©ployÃ©e :** [https://flask-projet-natixis-mlg.onrender.com](https://flask-projet-natixis-mlg.onrender.com)

**DÃ©veloppement local :** `http://localhost:5000`

## ğŸ“ Structure du Dossier

```
tests/
â”œâ”€â”€ __init__.py              # Package Python
â”œâ”€â”€ README.md                # Ce fichier (documentation principale)
â”œâ”€â”€ README_TESTS.md          # Documentation dÃ©taillÃ©e des tests
â”œâ”€â”€ RESUME_TESTS.md          # RÃ©sumÃ© et mÃ©triques des tests
â”œâ”€â”€ test_app.py              # Fichier principal de tests (40 tests)
â”œâ”€â”€ run_tests.py             # Script d'exÃ©cution automatique
â”œâ”€â”€ pytest.ini              # Configuration pytest
â””â”€â”€ .pytest_cache/          # Cache pytest (gÃ©nÃ©rÃ© automatiquement)
```

## ğŸš€ ExÃ©cution Rapide

### Depuis le dossier racine du projet
```bash
# Tous les tests
python tests/run_tests.py

# Tests rapides seulement
python tests/run_tests.py --fast

# Tests de performance seulement
python tests/run_tests.py --performance

# Tests avec couverture de code
python tests/run_tests.py --coverage
```

### Depuis le dossier tests
```bash
cd tests

# Tous les tests
python run_tests.py

# Tests spÃ©cifiques
python run_tests.py --unit
python run_tests.py --integration
```

## ğŸ“Š Tests Disponibles

### âœ… Tests de Pages Web (8 tests)
- Page d'accueil (`/`)
- Page CV (`/cv-mathis-le-gall`)
- Page indices et actions (`/indices-actions`)
- Page Call & Put (`/call-put`)
- Page surface de volatilitÃ© (`/volatility-surface`)
- Pages de debug (`/debug-finnhub`, `/debug-polygon`)
- Pages d'analyse (`/analyse-actions-indices`, `/description-app`)

### âœ… Tests d'APIs (15 tests)
- **Market Data** (`/api/market-data`)
- **Chart Data** (`/api/chart-data-v2/<symbol>`)
- **Calculate Option** (`/api/calculate-option`)
- **Risk Metrics** (`/api/risk-metrics/<symbol>`)
- **Volatility Surface** (`/api/vol-surface/<symbol>`)
- **Available Symbols** (`/api/available-symbols`)
- **Indices/Stocks** (`/api/indices`, `/api/stocks`)

### âœ… Tests de Calculs Financiers (8 tests)
- Options Call & Put avec diffÃ©rents paramÃ¨tres
- Grecques (Delta, Gamma, Theta, Vega, Rho)
- Monte Carlo vs Black-Scholes
- Intervalles de confiance
- Chemins Monte Carlo
- Haute volatilitÃ© et maturitÃ©s courtes

### âœ… Tests de Performance (2 tests)
- Temps de calcul < 10 secondes
- RequÃªtes concurrentes (5 simultanÃ©es)

### âœ… Tests de Gestion d'Erreurs (3 tests)
- JSON invalide
- Content-Type manquant
- Valeurs extrÃªmes

### âœ… Tests de Validation (4 tests)
- ParamÃ¨tres invalides pour les APIs
- Gestion des erreurs 404
- Routes legacy

## ğŸ¯ Classes de Test

### 1. `FlaskAppTestCase`
Tests des fonctionnalitÃ©s principales de l'application :
- Pages web
- APIs REST
- Calculs financiers

### 2. `PerformanceTestCase`
Tests de performance et de charge :
- Temps de calcul
- RequÃªtes concurrentes
- Mesure des performances

### 3. `ErrorHandlingTestCase`
Tests de robustesse et gestion d'erreurs :
- Gestion des erreurs
- Validation des donnÃ©es
- Cas limites

## ğŸ”§ Configuration

### Fichier `pytest.ini`
Configuration automatique pour pytest :
- Mode verbeux par dÃ©faut
- Gestion des marqueurs (slow, integration, unit, performance)
- Filtrage des avertissements
- Couleurs activÃ©es

### Mocking des APIs
Les tests utilisent `unittest.mock` pour simuler :
- âœ… Yahoo Finance API
- âœ… Finnhub API
- âœ… Polygon.io API

Cela permet de tester l'application sans dÃ©pendre des APIs externes.

## ğŸ“ˆ MÃ©triques de Test

### Taux de RÃ©ussite : 100%
- **40 tests** exÃ©cutÃ©s avec succÃ¨s
- **0 Ã©chec**
- **0 erreur**
- **Temps d'exÃ©cution** : ~8-10 secondes

### Couverture de Code
```bash
python tests/run_tests.py --coverage
```
- **Tests unitaires** : 35 tests
- **Tests d'intÃ©gration** : 3 tests
- **Tests de performance** : 2 tests
- **Couverture estimÃ©e** : >90%

## ğŸ› ï¸ Commandes Utiles

### ExÃ©cution de Base
```bash
# Tous les tests
python tests/run_tests.py

# Tests rapides (exclure les tests lents)
python tests/run_tests.py --fast

# Tests unitaires seulement
python tests/run_tests.py --unit

# Tests de performance seulement
python tests/run_tests.py --performance

# Tests d'intÃ©gration seulement
python tests/run_tests.py --integration
```

### Avec Couverture
```bash
# Tests avec couverture de code
python tests/run_tests.py --coverage

# Rapport HTML gÃ©nÃ©rÃ© dans tests/htmlcov/index.html
```

### Commandes Directes
```bash
cd tests

# Avec unittest
python test_app.py

# Avec pytest
python -m pytest test_app.py -v

# Tests spÃ©cifiques
python -m pytest test_app.py::FlaskAppTestCase -v
python -m pytest test_app.py::PerformanceTestCase -v
```

## ğŸ› DÃ©pannage

### Erreurs courantes

#### 1. ModuleNotFoundError
```bash
# Installer les dÃ©pendances
pip install flask pandas numpy scipy requests pytest coverage
```

#### 2. ImportError pour les modules locaux
```bash
# VÃ©rifier que vous Ãªtes dans le bon rÃ©pertoire
ls app.py tests/test_app.py
```

#### 3. Tests qui Ã©chouent
```bash
# VÃ©rifier les logs dÃ©taillÃ©s
cd tests
python -m pytest test_app.py -v -s
```

### Debug des tests
```bash
cd tests

# Mode debug avec pytest
python -m pytest test_app.py -v -s --pdb

# Test spÃ©cifique avec debug
python -m pytest test_app.py::FlaskAppTestCase::test_api_calculate_option_valid_data -v -s --pdb
```

## ğŸ“ Ajout de Nouveaux Tests

### 1. Test unitaire simple
```python
def test_nouvelle_fonctionnalite(self):
    """Test de la nouvelle fonctionnalitÃ©"""
    # Arrange
    test_data = {...}
    
    # Act
    response = self.app.post('/api/nouvelle-api', 
                           data=json.dumps(test_data),
                           content_type='application/json')
    
    # Assert
    self.assertEqual(response.status_code, 200)
    data = json.loads(response.data)
    self.assertIn('expected_field', data)
```

### 2. Test avec mock
```python
@patch('api.external_api.get_data')
def test_api_avec_mock(self, mock_get_data):
    """Test avec mock d'API externe"""
    mock_get_data.return_value = {'test': 'data'}
    
    response = self.app.get('/api/test')
    self.assertEqual(response.status_code, 200)
```

### 3. Test de performance
```python
def test_performance_nouvelle_fonction(self):
    """Test de performance"""
    start_time = time.time()
    
    # ExÃ©cuter la fonction
    result = self.app.post('/api/calcul-lourd', ...)
    
    end_time = time.time()
    self.assertLess(end_time - start_time, 5.0)  # < 5 secondes
```

## ğŸ¯ Bonnes Pratiques

### 1. Nommage des tests
- `test_<fonctionnalitÃ©>_<scÃ©nario>` : `test_api_calculate_option_valid_data`
- `test_<fonctionnalitÃ©>_<erreur>` : `test_api_calculate_option_invalid_data`

### 2. Structure AAA
- **Arrange** : PrÃ©parer les donnÃ©es de test
- **Act** : ExÃ©cuter l'action Ã  tester
- **Assert** : VÃ©rifier les rÃ©sultats

### 3. Isolation
- Chaque test doit Ãªtre indÃ©pendant
- Utiliser `setUp()` et `tearDown()` pour la configuration
- Nettoyer les donnÃ©es aprÃ¨s chaque test

### 4. Mocking
- Mocker les APIs externes
- Mocker les appels de base de donnÃ©es
- Mocker les fonctions coÃ»teuses

## ğŸ”„ IntÃ©gration Continue

Pour intÃ©grer dans un pipeline CI/CD :

```yaml
# .github/workflows/tests.yml
name: Tests
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.9
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Run tests
        run: python tests/run_tests.py
```

## ğŸ“š Documentation ComplÃ¨te

- **[README_TESTS.md](README_TESTS.md)** - Documentation dÃ©taillÃ©e des tests
- **[RESUME_TESTS.md](RESUME_TESTS.md)** - RÃ©sumÃ© et mÃ©triques des tests

## ğŸ“ Support

En cas de problÃ¨me avec les tests :
1. VÃ©rifier que toutes les dÃ©pendances sont installÃ©es
2. VÃ©rifier que vous Ãªtes dans le bon rÃ©pertoire
3. ExÃ©cuter avec `-v` pour plus de dÃ©tails
4. Consulter les logs d'erreur spÃ©cifiques

---

**Auteur** : Mathis Le Gall  
**Date** : 2024  
**Version** : 1.0  
**Statut** : âœ… TerminÃ© et ValidÃ©

