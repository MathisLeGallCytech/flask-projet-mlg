# ğŸ§ª Tests de l'Application Flask - Mathis Le Gall

Ce document dÃ©crit le systÃ¨me de tests complet pour l'application Flask de trading et d'analyse financiÃ¨re.

## ğŸ“‹ Vue d'ensemble

Le systÃ¨me de tests couvre :
- âœ… **Tests unitaires** : FonctionnalitÃ©s individuelles
- âœ… **Tests d'intÃ©gration** : Interactions entre composants
- âœ… **Tests de performance** : Temps de rÃ©ponse et charge
- âœ… **Tests d'API** : Endpoints REST
- âœ… **Tests de gestion d'erreurs** : Robustesse de l'application

## ğŸŒ AccÃ¨s Ã  l'Application

**Application dÃ©ployÃ©e :** [https://flask-projet-mlg.onrender.com](https://flask-projet-mlg.onrender.com)

**DÃ©veloppement local :** `http://localhost:5000`

## ğŸš€ ExÃ©cution des Tests

### MÃ©thode 1 : Script automatique (RecommandÃ©)

```bash
# Tous les tests
python run_tests.py

# Tests rapides seulement
python run_tests.py --fast

# Tests de performance seulement
python run_tests.py --performance

# Tests unitaires seulement
python run_tests.py --unit

# Tests d'intÃ©gration seulement
python run_tests.py --integration

# Tests avec couverture de code
python run_tests.py --coverage
```

### MÃ©thode 2 : Commandes directes

```bash
# Avec unittest (mÃ©thode native)
python test_app.py

# Avec pytest (plus de fonctionnalitÃ©s)
python -m pytest test_app.py -v

# Tests spÃ©cifiques
python -m pytest test_app.py::FlaskAppTestCase -v
python -m pytest test_app.py::PerformanceTestCase -v
python -m pytest test_app.py::ErrorHandlingTestCase -v
```

### MÃ©thode 3 : Avec couverture de code

```bash
# Installer coverage
pip install coverage

# ExÃ©cuter les tests avec couverture
coverage run -m pytest test_app.py

# GÃ©nÃ©rer le rapport
coverage report
coverage html  # GÃ©nÃ¨re htmlcov/index.html
```

## ğŸ“Š Structure des Tests

### 1. FlaskAppTestCase
Tests des fonctionnalitÃ©s principales de l'application :

#### Pages Web
- âœ… Page d'accueil (`/`)
- âœ… Page CV (`/cv-mathis-le-gall`)
- âœ… Page indices et actions (`/indices-actions`)
- âœ… Page Call & Put (`/call-put`)
- âœ… Page surface de volatilitÃ© (`/volatility-surface`)
- âœ… Pages de debug (`/debug-finnhub`, `/debug-polygon`)
- âœ… Pages d'analyse (`/analyse-actions-indices`)

#### APIs
- âœ… **Market Data** (`/api/market-data`)
  - Test avec donnÃ©es valides
  - Test avec erreur d'API
- âœ… **Chart Data** (`/api/chart-data-v2/<symbol>`)
  - Test avec donnÃ©es valides
  - Test avec erreur d'API
- âœ… **Calculate Option** (`/api/calculate-option`)
  - Test avec donnÃ©es valides (call/put)
  - Test avec donnÃ©es invalides
  - Test avec champs manquants
  - Test avec haute volatilitÃ©
  - Test avec maturitÃ© courte
  - Test avec nombre Ã©levÃ© de simulations
- âœ… **Risk Metrics** (`/api/risk-metrics/<symbol>`)
  - Test avec donnÃ©es valides
  - Test sans donnÃ©es
- âœ… **Volatility Surface** (`/api/vol-surface/<symbol>`)
  - Test avec paramÃ¨tres invalides
- âœ… **Available Symbols** (`/api/available-symbols`)
- âœ… **Indices/Stocks** (`/api/indices`, `/api/stocks`)

#### Calculs AvancÃ©s
- âœ… **Grecques** (Delta, Gamma, Theta, Vega, Rho)
- âœ… **Intervalles de confiance**
- âœ… **Chemins Monte Carlo**
- âœ… **Comparaison Black-Scholes vs Monte Carlo**

### 2. PerformanceTestCase
Tests de performance et de charge :

- âœ… **Temps de calcul** : VÃ©rification que les calculs d'options prennent moins de 10 secondes
- âœ… **RequÃªtes concurrentes** : Test avec 5 requÃªtes simultanÃ©es
- âœ… **Mesure des temps** : Rapport des temps de calcul Monte Carlo vs Black-Scholes

### 3. ErrorHandlingTestCase
Tests de robustesse et gestion d'erreurs :

- âœ… **JSON invalide** : Gestion des requÃªtes malformÃ©es
- âœ… **Content-Type manquant** : Validation des en-tÃªtes
- âœ… **Valeurs extrÃªmes** : Gestion des paramÃ¨tres hors limites

## ğŸ”§ Configuration

### Fichier pytest.ini
Configuration automatique pour pytest :
- Mode verbeux par dÃ©faut
- Gestion des marqueurs (slow, integration, unit, performance)
- Filtrage des avertissements
- Couleurs activÃ©es

### Mocking des APIs
Les tests utilisent `unittest.mock` pour simuler :
- âœ… Yahoo Finance API
- âœ… Finnhub API
- âœ… Polygon.io API

Cela permet de tester l'application sans dÃ©pendre des APIs externes.

## ğŸ“ˆ MÃ©triques de Test

### Couverture de Code
```bash
python run_tests.py --coverage
```

Cela gÃ©nÃ¨re :
- Rapport console avec pourcentage de couverture
- Rapport HTML dans `htmlcov/index.html`

### Performance
- â±ï¸ Temps de calcul d'options < 10 secondes
- ğŸ”„ Support de requÃªtes concurrentes
- ğŸ“Š Mesure des temps de rÃ©ponse

## ğŸ› DÃ©pannage

### Erreurs courantes

#### 1. ModuleNotFoundError
```bash
# Installer les dÃ©pendances
pip install flask pandas numpy scipy requests
```

#### 2. ImportError pour les modules locaux
```bash
# VÃ©rifier que vous Ãªtes dans le bon rÃ©pertoire
ls app.py test_app.py
```

#### 3. Tests qui Ã©chouent
```bash
# VÃ©rifier les logs dÃ©taillÃ©s
python -m pytest test_app.py -v -s
```

### Debug des tests

```bash
# Mode debug avec pytest
python -m pytest test_app.py -v -s --pdb

# Test spÃ©cifique avec debug
python -m pytest test_app.py::FlaskAppTestCase::test_api_calculate_option_valid_data -v -s --pdb
```

## ğŸ“ Ajout de Nouveaux Tests

### 1. Test unitaire simple
```python
def test_nouvelle_fonctionnalite(self):
    """Test de la nouvelle fonctionnalitÃ©"""
    # Arrange
    test_data = {...}
    
    # Act
    response = self.app.post('/api/nouvelle-api', 
                           data=json.dumps(test_data),
                           content_type='application/json')
    
    # Assert
    self.assertEqual(response.status_code, 200)
    data = json.loads(response.data)
    self.assertIn('expected_field', data)
```

### 2. Test avec mock
```python
@patch('api.external_api.get_data')
def test_api_avec_mock(self, mock_get_data):
    """Test avec mock d'API externe"""
    mock_get_data.return_value = {'test': 'data'}
    
    response = self.app.get('/api/test')
    self.assertEqual(response.status_code, 200)
```

### 3. Test de performance
```python
def test_performance_nouvelle_fonction(self):
    """Test de performance"""
    start_time = time.time()
    
    # ExÃ©cuter la fonction
    result = self.app.post('/api/calcul-lourd', ...)
    
    end_time = time.time()
    self.assertLess(end_time - start_time, 5.0)  # < 5 secondes
```

## ğŸ¯ Bonnes Pratiques

### 1. Nommage des tests
- `test_<fonctionnalitÃ©>_<scÃ©nario>` : `test_api_calculate_option_valid_data`
- `test_<fonctionnalitÃ©>_<erreur>` : `test_api_calculate_option_invalid_data`

### 2. Structure AAA
- **Arrange** : PrÃ©parer les donnÃ©es de test
- **Act** : ExÃ©cuter l'action Ã  tester
- **Assert** : VÃ©rifier les rÃ©sultats

### 3. Isolation
- Chaque test doit Ãªtre indÃ©pendant
- Utiliser `setUp()` et `tearDown()` pour la configuration
- Nettoyer les donnÃ©es aprÃ¨s chaque test

### 4. Mocking
- Mocker les APIs externes
- Mocker les appels de base de donnÃ©es
- Mocker les fonctions coÃ»teuses

## ğŸ“Š Rapports et MÃ©triques

### ExÃ©cution complÃ¨te
```bash
python run_tests.py
```

RÃ©sultat attendu :
```
ğŸš€ DÃ©marrage des tests de l'application Flask...
============================================================
test_home_page (__main__.FlaskAppTestCase) ... ok
test_cv_page (__main__.FlaskAppTestCase) ... ok
...
ğŸ“Š RÃ©sumÃ© des tests:
   Tests exÃ©cutÃ©s: 45
   Ã‰checs: 0
   Erreurs: 0
   SuccÃ¨s: 45

âœ… Tous les tests ont rÃ©ussi!
```

### Couverture de code
```bash
python run_tests.py --coverage
```

RÃ©sultat attendu :
```
Name                    Stmts   Miss  Cover
-------------------------------------------
app.py                    450     45    90%
test_app.py              280      0   100%
```

## ğŸ”„ IntÃ©gration Continue

Pour intÃ©grer dans un pipeline CI/CD :

```yaml
# .github/workflows/tests.yml
name: Tests
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.9
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Run tests
        run: python run_tests.py
```

## ğŸ“ Support

En cas de problÃ¨me avec les tests :
1. VÃ©rifier que toutes les dÃ©pendances sont installÃ©es
2. VÃ©rifier que vous Ãªtes dans le bon rÃ©pertoire
3. ExÃ©cuter avec `-v` pour plus de dÃ©tails
4. Consulter les logs d'erreur spÃ©cifiques

---

**Auteur** : Mathis Le Gall  
**Date** : 2024  
**Version** : 1.0
